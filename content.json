[{"title":"spark运行时的版本问题","date":"2017-11-19T15:00:43.000Z","path":"2017/11/19/spark运行时的版本问题/","text":"第一次运行的时候，报了这样的一个错误Exception in thread \"main\" java.lang.NoSuchMethodError: scala.Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps; at org.apache.spark.util.Utils$.getCallSite(Utils.scala:1440) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:76) at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:58) at com.xiaojiezhu.spark.rdd.wordcount.JavaWordCount.main(JavaWordCount.java:26) 报错的位置是 很奇怪，这一行为什么要报错呢 原因是这样的spark的版本要与scala的版本一致才行，我们回头看一下spark的maven坐标 &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt; 我们仔细看一下artifactId里面的属性，后面是带了一个版本号的2.10，这个版本号就是spark对应的scala版本，需要版本相对应才能正确运行 于是乎，我卸载了scala，在scala官网找到相应的2.10的版本重新安装，然后就好了","tags":[{"name":"spark问题","slug":"spark问题","permalink":"http://blog.xiaojiezhu/tags/spark问题/"}]},{"title":"spark的wordcount","date":"2017-11-19T12:52:49.000Z","path":"2017/11/19/spark的wordcount/","text":"基本上学习大数据的第一个应用程序，都是一个wordcount程序，也就是统计一个文本里面出现的单词次数 第一个wordcount以英文单词来演示，因为中文是以词语来切分才会有意思，如果中文切分词语，又要用到分词技术，分词并不是本文的重点，如果想要了解中文分词，请参考作者的另一篇文章hadoop的wordcount，这里面有详细的中文分词，甚至统计了斗破苍穹这本小说哪些词语出现的最多，你们猜恐怖如斯出现了多少次呢 准备一个文本文本内容如下hello worldhello janehello jiehello kangkangfuck youi am god 如果运行过程中报java.lang.NoSuchMethodError，请查看spark运行时的版本问题 wordcount的java实现普通java代码的实现import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import scala.Tuple2;import java.util.Arrays;import java.util.Iterator;/** * @Author 朱小杰 * 时间 2017-09-23 .22:31 * 说明 ... */public class JavaWordCount &#123; public static void main(String[] args) &#123; String dir = \"D:/spark/workcount/\"; SparkConf conf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); JavaSparkContext sc = new JavaSparkContext(conf); JavaRDD&lt;String&gt; input = sc.textFile(dir + \"in/text.txt\"); //切分为单词 JavaRDD&lt;String&gt; words = input.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterator&lt;String&gt; call(String s) throws Exception &#123; return Arrays.asList(s.split(\" \")).iterator(); &#125; &#125;); //转换为键值对并计数 JavaPairRDD&lt;String, Integer&gt; counts = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String s) throws Exception &#123; return new Tuple2&lt;&gt;(s, 1); &#125; &#125;).reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer x, Integer y) throws Exception &#123; return x + y; &#125; &#125;); //保存为文本文件 counts.saveAsTextFile(dir + \"result\"); &#125;&#125; lambda表达式的实现普通java代码看着麻烦了一点，我们看一下lambdaimport org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import scala.Tuple2;import java.util.Arrays;import java.util.Iterator;/** * @Author 朱小杰 * 时间 2017-09-23 .22:31 * 说明 ... */public class JavaWordCount &#123; public static void main(String[] args) &#123; String dir = \"D:/spark/workcount/\"; SparkConf conf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); JavaSparkContext sc = new JavaSparkContext(conf); JavaRDD&lt;String&gt; input = sc.textFile(dir + \"in/text.txt\"); JavaRDD&lt;String&gt; words = input.flatMap(s -&gt; Arrays.asList(((String) s).split(\" \")).iterator());//java8 表达式版 JavaPairRDD&lt;String, Integer&gt; counts = words.mapToPair(s -&gt; new Tuple2&lt;&gt;(s, 1)).reduceByKey((x, y) -&gt; x + y); //使用java8的语法 counts.saveAsTextFile(dir + \"result\"); &#125;&#125; wordcount的scala实现import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * 朱小杰 * 时间 2017-09-24 .9:32 * 说明 ... */object ScalaWordCount &#123; def main(args : Array[String]): Unit =&#123; val conf = new SparkConf().setMaster(\"local\").setAppName(\"wordcount\") val sc = new SparkContext(conf) val dir = \"G:\\\\javacode\\\\workspace\\\\spark\\\\spark-rdd\\\\src\\\\main\\\\java\\\\com\\\\xiaojiezhu\\\\spark\\\\rdd\\\\wordcount\\\\\" val input = sc.textFile(dir + \"wordcount.txt\") val words = input.flatMap(line =&gt; line.split(\" \")) val counts = words.map(word =&gt; (word,1)).reduceByKey((x,y) =&gt; x + y) counts.saveAsTextFile(dir + \"result\") &#125;&#125; 执行效果 在指定生成的目录会有下面几个文件 _SUCCESS文件代表着执行成功 part-00000文件里面有最终的返回内容，内容如下(jane,1)(you,1)(kangkang,1)(god,1)(am,1)(i,1)(jie,1)(hello,4)(fuck,1)(world,1) 这里面就有着每个单词分别出现了多少次，像hello就出现了4次","tags":[{"name":"spark","slug":"spark","permalink":"http://blog.xiaojiezhu/tags/spark/"},{"name":"大数据","slug":"大数据","permalink":"http://blog.xiaojiezhu/tags/大数据/"}]},{"title":"在开发工具中编写spark代码","date":"2017-11-19T04:54:04.000Z","path":"2017/11/19/在开发工具中编写spark代码/","text":"我们编写spark代码时，肯定不会在shell中，我们都是在编译工具中操作的 使用MAVEN工程使用spark的API很简单，引入spark的依赖就好了&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;$&#123;version&#125;&lt;/version&gt;&lt;/dependency&gt; spark的依赖中包含了一个spark的运行环境，也就是说，可以直接在IDEA以本地模式运行，这样也方便调试，也不需要连接一个服务器上面的spark环境 初始化SparkContextspark的一切操作都是从SparkContext开始的 在java中初始化 有必要值得一提的是，下面代码中连接的local值是一个特殊值，它代表着会在本机启动一个spark的环境，如果要连接远程spark机器，则填写远程服务器地址 appName是给当前应用起一个名字，让管理员好区分是哪一个应用 import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaSparkContext;/** * @Author 朱小杰 * 时间 2017-09-23 .22:23 * 说明 使用scala代码初始化SparkContext &lt;br&gt; * setMaster() 设置集群url，local这个特殊值可以运行在单机线程中而无需连接集群&lt;br&gt; * setAppName() 设置应用的名称，使得可在集群管理器中可以找到这个应用的名称&lt;br&gt; */public class JavaSparkContextInit &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf().setMaster(\"local\").setAppName(\"my App\"); JavaSparkContext jsc = new JavaSparkContext(conf); &#125;&#125; 在scala中初始化import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * 朱小杰 &lt;br&gt; * 时间 2017-09-23 .22:08&lt;br&gt; * 说明 使用scala代码初始化SparkContext &lt;br&gt; * setMaster() 设置集群url，local这个特殊值可以运行在单机线程中而无需连接集群&lt;br&gt; * setAppName() 设置应用的名称，使得可在集群管理器中可以找到这个应用的名称&lt;br&gt; */object ScalaSparkContextInit &#123; def main(args:Array[String]): Unit =&#123; val conf = new SparkConf().setMaster(\"local\").setAppName(\"my app\") val sc = new SparkContext(conf) &#125;&#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://blog.xiaojiezhu/tags/spark/"},{"name":"大数据","slug":"大数据","permalink":"http://blog.xiaojiezhu/tags/大数据/"}]},{"title":"下载spark","date":"2017-11-19T03:50:29.000Z","path":"2017/11/19/下载spark/","text":"下载并使用spark 本章节仅仅是下载，并且介绍spark的目录，是普通安装模式，并不是集群模式，集群模式请参见spark集群模式安装 在官网下载spark的压缩包，或者从github中clone代码，然后自行编译。 解压出来有如下目录 bin 包含了可以和spark进行各种交互的shell example 官网提供的一些示例，可以查看并学习它的API conf spark的配置 jars 包含了spark运行的所有需要的jar包 sbin 真正启动spark的运行脚本 spark的shellspark带有交互式的shell，可以做即时的数据分析，提供了R，Python,Scala所提供的shell 为什么要有spark的shell，因为spark可以在多台计算机中并行计算，所以很多分布式计算都可以在几秒钟之内完成，哪握是那种十几个节点处理TB级别数据的计算，这种情况spark shell就很适合这种情况，因为不需要编写代码运行，可以在shell中直接处理并且查看消息，不仅仅是与本机的连接，甚至是远程spark机器的连接，包括连接spark集群 spark的日志有很多，很杂，如果想要调整日志的输出限制，则在conf目录创建一个log4j.properties文件来管理日志的设置，spark的conf目录已经有一个log4j.properties.template文件了，我们复制它修改这个文件就行了spark的shell在window运行有点问题，需要安装点东西，建议在linux环境运行，或者安装个虚拟机 python的shell运行python的shell的方式，进行spark根 目录，运行bin/pyspark，就会进入python的shell笔者对python不熟悉，所以以scala为例 spark的scala运行bin/spark-shell 得到如下界面 这就是spark的命令行了 我们尝试一个简单的行数统计统计一个文本文件有多少行，我们在spark的shell中输入如下命令var lines = sc.textFile(\"d:/spark/text.txt\");lines.count();//得到这个文件有多少行lines.first();//获取这个rdd中的第一个元素，也就是第一行 在如下的例子中，lines是一个rdd，是从本机电脑的文件中创建出来的，我们可以通过rdd进行各种运算","tags":[{"name":"spark","slug":"spark","permalink":"http://blog.xiaojiezhu/tags/spark/"},{"name":"大数据","slug":"大数据","permalink":"http://blog.xiaojiezhu/tags/大数据/"}]},{"title":"spark介绍","date":"2017-11-19T02:50:13.000Z","path":"2017/11/19/spark介绍/","text":"spark系列spark是什么spark是一个通用计算框架，包含了一整套的计算模型，以及布式式集群的资源调度 spark的特点与hadoop相比，spark在内存中计算，速度比hadoop快的多，便于进行交互式操作 spark提供了python,java,scala,sql的API,并且spark还能运行在hadoop集群 spark的组件 spark core spark sql spark streaming MLlib GraphX spark core实现了spark core的基本功能，包含任务调度，内存管理，错误恢复，与存储系统的交互模块 spark sqlspark sql是spark用来操作数据结构化的程序包，通过spark sql，我们可以通过hive版本的sql来查询数据 spark streamingspark streaming是spark提供的对实时数据流计算的组件，比如网页服务吕在日志，或者用户提供的状态组成的消息队列，spark streaming提供了用来操作数据流的API，并且与RDD高度对应 MLlibspark包含了常见的机器学习算法，包括分类，回归，聚类，协同过滤等，还提供了模型评估，数据导入等额外的支持功能 GraphX这是用来操作图，比如社交网络的朋友关系图的程序库，可以进行并行的图计算，也扩展了rdd的API","tags":[{"name":"spark","slug":"spark","permalink":"http://blog.xiaojiezhu/tags/spark/"},{"name":"大数据","slug":"大数据","permalink":"http://blog.xiaojiezhu/tags/大数据/"}]},{"title":"浏览器鼠标滚轮事件","date":"2017-11-18T15:47:42.000Z","path":"2017/11/18/浏览器鼠标滚轮事件/","text":"监听浏览器的滚轮事件实现目标 监听鼠标滚轮事件 获取是向上滚动还是向下滚动 处理鼠标的连续滚动 笔者以谷歌浏览器亲试，谷歌内核的都可以火狐浏览器不行，ie浏览器也不行 1.实现鼠标滚轮事件window.onmousewheel = function(e)&#123; console.log(e); if(e.deltaY &gt; 0)&#123; //向下 &#125;else&#123; //向上 &#125;&#125; 2.获取是向上滚动还是向下滚动 这里面的deltaY是Y轴滚动的像素点，如果是正数，则是向下滚动，如果是负数，则是向下滚动 3.处理鼠标滚动的连续滚动事件 先说一下什么是鼠标的连续滚动，鼠标的滚轮每转动到相应的地方都会触发一个滚轮事件，但是如果滚动的用力，就可能会触发多次的滚动事件。也就是说操作一次鼠标滚动，很有可能会触发多次鼠标滚动的事件，这样如果要处理一些数据的事件，就不是我们想要的效果了 我们来处理掉这个事件，让它1秒钟只触发一次就行了 window.onmousewheel = function(e)&#123; console.log(JSON.stringify(e)); if(e.deltaY &gt; 0)&#123; //向下 if(enableRool())&#123; //做点事情 &#125; &#125;else&#123; //向上 if(enableRool())&#123; //做点事情 &#125; &#125;&#125;var roolTime = new Date().getTime();function enableRool()&#123; var now = new Date().getTime(); if(now - roolTime &gt; 1000)&#123; roolTime = now; return true; &#125; return false;&#125; 通过如上的代码可以看到，在enableRool()方法中，如果距离上次滚动时间小于1秒的话，是不会在触发滚轮事件的，所以也就不会造成一次操作，触发多次滚动的事件了","tags":[{"name":"js","slug":"js","permalink":"http://blog.xiaojiezhu/tags/js/"},{"name":"事件","slug":"事件","permalink":"http://blog.xiaojiezhu/tags/事件/"}]},{"title":"第一篇文章","date":"2017-11-13T02:26:00.000Z","path":"2017/11/13/writeblog/","text":"怎么写一篇博客呢你猜啊 是的，它就是这样子的: 直接量 变量 代码如下public void main(String[] args)&#123; System.out.println(args);&#125;","tags":[{"name":"文章标签","slug":"文章标签","permalink":"http://blog.xiaojiezhu/tags/文章标签/"}]},{"title":"Hello World","date":"2017-11-13T02:11:23.637Z","path":"2017/11/13/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","tags":[]},{"title":"spring的factorybean","date":"2017-11-12T16:00:00.000Z","path":"2017/11/13/spring-factorybean/","text":"spring的factory是什么作用呢FactoryBean 可以让用户把任意bean注入到spring中","tags":[{"name":"spring","slug":"spring","permalink":"http://blog.xiaojiezhu/tags/spring/"}]},{"title":"vue的用法","date":"2017-11-12T16:00:00.000Z","path":"2017/11/13/vue/","text":"#如何使用vue var v = new Vue(&#123; el : \"#app\", data : &#123;&#125;&#125;); &lt;html&gt; &lt;head&gt;&lt;/head&gt; &lt;body&gt;&lt;/body&gt;&lt;/html&gt;","tags":[{"name":"vue","slug":"vue","permalink":"http://blog.xiaojiezhu/tags/vue/"}]}]